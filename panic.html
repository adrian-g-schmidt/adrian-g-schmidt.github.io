---
permalink: /portfolio/panic
---

<!DOCTYPE html>
<html>

<head>
    <title>Adrian Schmidt</title>
    <link rel="icon" type="image/svg+xml" href="/images/icon.svg">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" href="/assets/css/big-portfolio.css">
</head>

<body>

    <div id="navbar" class="smaller-text">
        <a href="/">
            <h2>
                <span class="l1">A</span><span class="w1">DRIAN</span>
                <br>
                <span class="l2">S</span><span class="w2">CHMIDT</span>
            </h2>
        </a>
        
        <div>
            <div class="icon-parent">
                <a href="javascript:void(0);" class="icon" onclick="hamburger()">
                    <div class="ham"></div>
                    <div class="ham"></div>
                    <div class="ham"></div>
                </a>
            </div>
            <div id="links">
                <a href="/assets/adrian-schmidt-resume.pdf" style="font-weight:bold; padding: 0px 20px;">
                    Resume
                </a>
                <a href="/#portfolio" style="font-weight:bold; padding: 0px 20px;">
                        Portfolio
                </a>
                <a href="/#contact" style="font-weight:bold; padding: 0px 20px;">
                    Contact
                </a>
            </div>
         </div>


    </div>

    <div id="content">
        <div id="cover" style="background-image: url('/assets/images/panic/panic-banner.jpg'); ">
            <img id="title-small" src="/assets/images/panic/panic-text.svg">

            <figure>
                <video width="100%" controls>
                    <source src="/assets/images/panic/panic-win-video.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video>           
                <figcaption>Footage of Panic shown on Win News
                </figcaption>
            </figure>

            <p>
            <img id="title-big" src="/assets/images/panic/panic-text.svg">
            <strong>Role:</strong> Co-creator, Terminal Design, Fabrication and Assembly.
            <br><br>
            <strong>Tools:</strong> Fusion360, Illustrator Laser Cutter, Vinyl Cutter, 3D-Printers
            <br><br>
            <strong>Description:</strong> Panic (Playground AI Network for Interactive Creativity) is an interactive artwork. It takes a text input and runs it through a network of three separate AI models, displaying the outputs over six screens and split-flap displays. The first two models take a text input and generate an image representation. The third one “captions” the image, turning it back into text. This caption is fed back into the first model, and the cycle starts again. AI models exist within larger systems, and Panic demonstrates the cumulative effect of many different AI text-to-image-and-back iterations, which can lead to surprising – even serendipitous – places.
            </p>
        </div>

        <div id="study">

        <h2>
            <span class="l1">O</span><span class="w1">RIGIN <i>&</i></span>
            <br>
            <span class="l2">A</span><span class="w2">DAPTION</span>
        </h2>
        <p>
            I developed the original concept for Panic for the Creative AI Sydney conference with Dr Ben Swift, who did most of the programming. The program allowed the viewer to choose different AI models that could feed into each other. For example, the output from a voice-to-text AI could be fed into text-to-image AI, which could be fed into an image description model, and so on. It is a way to investigate the strengths and weaknesses of these AI models, and where their inherent biases lead. Audiences enjoyed seeing how each AI model would pick up or ignore details from the previous output.
            <figure>
                <img src="/assets/images/panic/ben-presenting-creative-ai.jpg" 
                alt="Ben Swift presenting the first version of Panic at Creative AI Sydney">
                <figcaption>Ben Swift presenting the first version of Panic at Creative AI Sydney
                </figcaption>
            </figure>
            Ben and I were asked to demonstrate generative AI models for the ANU School of Cybernetics Launch Exhibition ‘Australian Cybernetic: A Point Through Time’. The exhibit involves many creative computing displays from the 60s and 70s. We suggested Panic could be adapted to engage audiences with modern AI models. 
            <br>
            <br>
            To adapt Panic into an exhibit, we needed it to work un-facilitated and distributed over a larger area rather than contained to one screen. We chose Vestaboards - modern internet-controlled split-flaps displays - as they bring together old and new technology. The dynamic and audio qualities of these displays, along with people’s familiarity from travel, made them an engaging display tool.
            <figure>
                <img src="/assets/images/panic/vestaboard.jpg" 
                alt="The split-flap display Vestaboard">
                <figcaption>The split-flap display Vestaboard</figcaption>
            </figure>
            I was in charge of developing and connecting the hardware. This included building an interface that the viewer uses to input their prompt, the screens used, and internet connections for each device.
        </p>

        <h2>
            <span class="l1">D</span><span class="w1">ESIGNING <i>the</i></span>
            <br>
            <span class="l2">T</span><span class="w2">ERMINAL</span>
        </h2>        <p>
            We explored two options for the viewer to input their prompt to being looping the AI models. We chose a keyboard input over a microphone using AI-powered speech-to-text. This allowed us to keep interaction as simple and accurate as possible in a noisy environment, where the visitors would have a range of accents. I replaced the enter key with a titular “PANIC!” button, as big red button are highly engaging. 
            <br>
            <br>
            To design the housing for the keyboard, I used the photogrammetry application ‘Polycam’. I could then import the 3D scanned keyboard into the CAD program Fusion 360 to be able to cover un-necessary keys.
            <figure>
                <img src="/assets/images/panic/keyboard-scan.jpg" 
                alt="A photoscanned 3d model of the keyboard">
                <figcaption>The photo-scanned keyboard shown in Fusion360.
                </figcaption>
            </figure>
            On testing, while the inset keys worked, more room was need to comfortably rest the typists palms. The single space-bar key also confused people, so I designed and 3D-printed a space-bar key cap that could sit over both keys.
            <figure>
                <div class="img-grid">
                <img src="/assets/images/panic/space-bar-fusion.jpg" 
                alt="A 3D model of the space bar key">
                <img src="/assets/images/panic/space-bar-attached.jpg" 
                alt="The keyboard with the new spacebar attached">
                </div>
                <figcaption>The 3D-printed space bar in Fusion360 and attached to the keyboard.
                </figcaption>
            </figure>
        
        </p>

        <h2>
            <span class="l1">P</span><span class="w1">ROGRAMMING <i>&</i></span>
            <br>
            <span class="l2">P</span><span class="w2">ROTOTYPING</span>
        </h2>        <p>
            I used a Raspberry Pi 4 as the computer to connect the Panic terminal to the web browser. This allowed me to wire the ‘PANIC!’ button into the GPIO pins, meaning I could control whether the internal light was on or off and detect when the button was pressed. I wrote a Python script that links the switch of the button to ‘enter’, and could turn the light on and off to signal when a new prompt could be entered. After testing, the time-out information was better displayed on the console screen. The red button came with a dim and central LED. To make it bright and consistently lit, I added reflective tape, vinyl cut letters, and a 5V LED strip.
            <figure>
                <video width="100%" controls>
                    <source src="/assets/images/panic/button-led.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video>           
                <figcaption>The final button lighting, with the light turning off when pressed.
                </figcaption>
            </figure>
            <figure>
                <img src="/assets/images/panic/cybersyn.jpg" 
                alt="The Cybersyn control room, designed by Stafford Beer">
                <figcaption>The Cybersyn control room, designed by Stafford Beer (Image from <a href='https://99percentinvisible.org/episode/project-cybersyn/'>99% Invisible</a>)
                </figcaption>
            </figure>
            I used a small monitor as the screen, and built the housing. Inspired by the Cybersyn control room, created by Cybernetician Stafford Beer, I designed the console using Fusion360. I 3D printed the frame and used a laser cutter to make the flat panels. I mounted the monitor to the frame, leaving some panels removable for easy access. 
            <figure>
                <div class="img-grid">
                <img src="/assets/images/panic/frame-fusion.jpg" 
                alt="The 3D model of the frame in Fusion360">
                <img src="/assets/images/panic/full-fusion.jpg" 
                alt="The 3D model of the full terminal in Fusion360">
                </div>
                <figcaption>The terminal design with the 3D-printed frame, and the full design shown in Fusion360
                </figcaption>
            </figure>
            Once the pieces were 3D-printed, I glued them together with super glue and attached the fixed acrylic panels, reinforcing the weaker connections on the inner side with epoxy and filled any gaps with a plastic filler. Once set, I sanded the whole body and spray-painted it with matte black paint. 
            <figure>
                <div class="img-grid">
                <img src="/assets/images/panic/3d-printing.jpg" 
                alt="A 3D printer printing the parts">
                <img src="/assets/images/panic/box-board-practice.jpg" 
                alt="A test of the terminal with a keyboard using the 3D printed parts and cardboard parts">
                </div>
                <figcaption>3D printing the frame, and testing the laser cut parts using box board.
                </figcaption>
            </figure>
            <figure>
                <div class="img-grid">
                <img src="/assets/images/panic/assembled-blank.jpg" 
                alt="The spray painted frame, with removable panels and monitor attached.">
                <img src="/assets/images/panic/terminal-first-on.jpg" 
                alt="The terminal with the Raspberry Pi turned on">
                </div>
                <figcaption>The spray painted frame, with removable panels and monitor attached, and the terminal with the Raspberry Pi turned on.
                </figcaption>
            </figure>
        </p>
        <h2>
            <span class="l1">S</span><span class="w1">ETTING <i>up the</i></span>
            <br>
            <span class="l2">S</span><span class="w2">YSTEM</span>
        </h2>        
        <p>
            I connected the three Vestaboards and televisions to the internet using ethernet cables and a switch. I side-loaded the web browsers to the televisions, so they could display the generated images as they came through.
            <br>
            <br>
            Testing the prototype, I noticed that the sunken keys affected people’s ability to type quickly and editing typos was slow. To fix this, I uncovered all the keys and raised the keyboard, realising I could re-map the new keys to the numbers. I made the top of the keys using a vinyl cutter and added two arrow keys to make correcting typos easy. I also swapped the semi-colon with an apostrophe, a punctuation mark a few people asked for. With these changes, viewers found it much easier to type their prompts.
            <figure>
                <img src="/assets/images/panic/keyboard-fixed.jpg" 
                alt="The final terminal design, with extra keys added.
                ">
                <figcaption>The final terminal design, with extra keys added.
                </figcaption>
            </figure>
            Watching viewers type a prompt in, press ‘PANIC!’ and laugh as they see how impressive and surprising the models are has been very satisfying. Panic quickly demonstrates these AI models’ current strengths and weaknesses while being entertaining and awe-inspiring. Here’s an example for the prompt ‘A Group of Dogs wearing red Hats and a Bow’, displayed on a poster designed by graphic designer Ken Wheeler.
            <figure>
                <img src="/assets/images/panic/panic-poster.jpg" 
                alt="Synthwave stylised poster showing an example where the input 'a group of dogs wearing hat and red bows' turns into 'teddy bears are dressed like soldiers'.">
                <figcaption>Poster for the artwork designed by Ken Wheeler
                </figcaption>
            </figure>
        </p>
        <h2>
            <span class="l1">R</span><span class="w1">ESULTS <i>&</i></span>
            <br>
            <span class="l2">R</span><span class="w2">EFLECTIONS</span>
        </h2>        
        <p>
            It was a great challenge creating this work in a short time period. I gained new skills using Fusion360, the vinyl cutter, and designing and fabricating an engaging interface that could hold up to public use.
            <br><br>
            Unexpected internet restrictions at the University posed considerable unexpected challenges. I solved these through persistence and negotiating with the University IT team to install an ethernet port among other changes. 
            <br><br>
            Design and fabrication of the terminal went smoothly because I had many points of user testing and iteration throughout the process. I’m very excited to further develop design and fabrication skills on future projects.
            <br><br>
            We are collecting data from the exhibit to track how these models morph from prompts to final output, to better understand the biases inherent in the algorithms. We have plans for Panic to tour outside the School of Cybernetics in 2023 and will also use it in workshops on AI creative tools delivered at the ANU School of Cybernetics. 
            <br><br>
            You can read more about 'Panic' in Ben's blogpost <a href="https://cybernetics.anu.edu.au/news/2022/11/22/panic-a-serendipity-engine/">on the ANU School of Cybernetics website</a>.
            
            <br><br>
            <strong>Project team:</strong> Ben Swift, Adrian Schmidt.
            <br>
            <br>
        </p>

        <div style="text-align: center; padding-bottom: 50px; font-size: 20px;">
        <a class ='noselect' href="/#portfolio">
            Return to Portfolio
        </a>
        </div>


        </div>
    </div>
    
</body>


<script src="/assets/js/words-anim.js"></script>


</html>
    
<!-- Some references are: Replicate, Stable diffusion, Australian Cybernetic, Cybersyn, Panic blogpost -->
